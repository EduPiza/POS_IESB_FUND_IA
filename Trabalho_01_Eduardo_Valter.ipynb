{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS IESB 2020 - Disciplina Fundamentos de IA\n",
    "# Trabalho 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aluno: Eduardo Gomes Piza\n",
    "## Aluno: Valter Takechi Hada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando a toolbox de sua preferência, realize o treinamento de uma rede neural artificial do tipo   Perceptron Multicamadas para resolver um problema de classificação multiclasses.\n",
    "\n",
    "### Informações: Utilizamos um dos datasets constantes no site \"archive.ics.uci/edu\" (conforme sugestão) cujo conteúdo está relacionado a \"Early stage diabetes risk prediction dataset\" (dados para previsão de risco de diabetes em estagio inicial). \n",
    "\n",
    "### Dados adicionais quanto ao dataset e arquivo .csv podem ser verificados através da url \"http://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.\"\n",
    "\n",
    "### O dataset contém respostas de um questionário respondido por pacientes do \"Sylhet Diabetes Hospital\", em Bangladesh, e foi aprovado por médico/especialista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1\n",
    "# Vamos importar o dataset com os dados que serão utilizados\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "uri = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00529/diabetes_data_upload.csv\"\n",
    "#uri = \"diabetes_data_upload.csv\"\n",
    "dados_input = pd.read_csv(uri)\n",
    "dados_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 2\n",
    "# Vamos atualizar todo o data set renomeando as colunas com o termo traduzido\n",
    "\n",
    "cabecalho = { \n",
    "    \"Age\" : \"Idade\",\n",
    "    \"Gender\" : \"Sexo\",\n",
    "    \"Polyuria\" : \"Poliuria\",\n",
    "    \"Polydipsia\" : \"Polidipsia\",\n",
    "    \"sudden weight loss\" : \"Perda_subita_de_peso\",\n",
    "    \"weakness\" : \"Fraqueza\",\n",
    "    \"Polyphagia\" : \"Polifagia\",\n",
    "    \"Genital thrush\" : \"Herpes_genital\",\n",
    "    \"visual blurring\" : \"Visao_borrada\",\n",
    "    \"Itching\" : \"Coceira\",\n",
    "    \"Irritability\" : \"Irritabilidade\",\n",
    "    \"delayed healing\" : \"Cicatrizacao_lenta\",\n",
    "    \"partial paresis\" : \"Paralisia_parcial\",\n",
    "    \"muscle stiffness\" : \"Rigidez_muscular\",\n",
    "    \"Alopecia\" : \"Calvicie\",\n",
    "    \"Obesity\" : \"Obesidade\",\n",
    "    \"class\" : \"Situacao\"\n",
    "}\n",
    "dados_input = dados_input.rename(columns = cabecalho)\n",
    "dados_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 3\n",
    "# Vamos atualizar o conteudo das linhas, transformando o conteúdo em números:\n",
    "\n",
    "converte_sexo = { \"Male\" : 1,\n",
    "                  \"Female\" : 0 }\n",
    "\n",
    "dados_input['Sexo'] = dados_input.Sexo.map(converte_sexo)\n",
    "\n",
    "converte_resp = { \"No\" : 0,\n",
    "                  \"Yes\" : 1 }\n",
    "\n",
    "dados_input['Poliuria'] = dados_input.Poliuria.map(converte_resp)\n",
    "dados_input['Polidipsia'] = dados_input.Polidipsia.map(converte_resp)\n",
    "dados_input['Perda_subita_de_peso'] = dados_input.Perda_subita_de_peso.map(converte_resp)\n",
    "dados_input['Fraqueza'] = dados_input.Fraqueza.map(converte_resp)\n",
    "dados_input['Polifagia'] = dados_input.Polifagia.map(converte_resp)\n",
    "dados_input['Herpes_genital'] = dados_input.Herpes_genital.map(converte_resp)\n",
    "dados_input['Visao_borrada'] = dados_input.Visao_borrada.map(converte_resp)\n",
    "dados_input['Coceira'] = dados_input.Coceira.map(converte_resp)\n",
    "dados_input['Irritabilidade'] = dados_input.Irritabilidade.map(converte_resp)\n",
    "dados_input['Cicatrizacao_lenta'] = dados_input.Cicatrizacao_lenta.map(converte_resp)\n",
    "dados_input['Paralisia_parcial'] = dados_input.Paralisia_parcial.map(converte_resp)\n",
    "dados_input['Rigidez_muscular'] = dados_input.Rigidez_muscular.map(converte_resp)\n",
    "dados_input['Calvicie'] = dados_input.Calvicie.map(converte_resp)\n",
    "dados_input['Obesidade'] = dados_input.Obesidade.map(converte_resp)\n",
    "\n",
    "converte_situacao = { \"Positive\" : 1,\n",
    "                      \"Negative\" : 0}\n",
    "\n",
    "dados_input['Situacao'] = dados_input.Situacao.map(converte_situacao)\n",
    "\n",
    "dados_input.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 4\n",
    "# Uma vez que os dados foram tratados vamos ordená-los de forma aleatória, \n",
    "# separar as colunas de classificação (features) e a coluna de resultado, e\n",
    "# ajustar o tipo de dado para usá-lo como input dos procedimentos.\n",
    "\n",
    "import numpy as np\n",
    "#from sklearn.utils import shuffle\n",
    "#dados_input = shuffle(dados_input)\n",
    "\n",
    "x = dados_input[['Idade', \n",
    "                 'Sexo',\n",
    "                 'Poliuria',\n",
    "                 'Polidipsia',\n",
    "                 'Perda_subita_de_peso',\n",
    "                 'Fraqueza',\n",
    "                 'Polifagia',\n",
    "                 'Herpes_genital',\n",
    "                 'Visao_borrada',\n",
    "                 'Coceira',\n",
    "                 'Irritabilidade',\n",
    "                 'Cicatrizacao_lenta',\n",
    "                 'Paralisia_parcial',\n",
    "                 'Rigidez_muscular',\n",
    "                 'Calvicie',\n",
    "                 'Obesidade']]\n",
    "\n",
    "y = dados_input[['Situacao']]\n",
    "\n",
    "# Setando a tipagem do conteudo como inteiro\n",
    "x = x.astype(np.uint8)\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "# convertendo de 'pandas.dataframe' para 'numpy.ndarray'\n",
    "x = x.to_numpy(copy=True)\n",
    "y = y.to_numpy(copy=True)\n",
    "\n",
    "print(type(x))\n",
    "print(x.shape)\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 05\n",
    "# Agora vamos separar o data set para termos os dados de TREINO e TESTE, \n",
    "# ordenando-os de forma aleatória e garantindo que os registros selecionados\n",
    "# para teste apresentem a mesma proporção que os registros de treino para \n",
    "# a 'SITUACAO' (que indica se o paciente possui ou nao diabetes) . Vamos \n",
    "# separar 80% dos registros para treino e 20% para testes. \n",
    "\n",
    "# Para separacao dos grupos de treino e teste usando 'train_test_split'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# métricas\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# validação cruzada\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# matrizes numéricas e plot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# organiza aleatoriamente matrizes \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# funcao para recuperacao do timestamp\n",
    "from time import process_time\n",
    "\n",
    "# Como vamos usar 'StratifiedKFold' para separar os registros de treino e teste\n",
    "# nao precisamos executar os procedimentos abaixo, que fariam o mesmo processo\n",
    "# mas usando a 'train_test_split'\n",
    "##SEED = 20\n",
    "##treino_x, teste_x, treino_y, teste_y = train_test_split(x, \n",
    "##                                                        y,\n",
    "##                                                        random_state = SEED, \n",
    "##                                                        test_size = 0.10,\n",
    "##                                                        stratify = y)\n",
    "##print(\"Treinaremos com %d elementos e testaremos com %d elementos\" % (len(treino_x), len(teste_x)))\n",
    "\n",
    "# Parametros que serao usados na rede\n",
    "epocas = 5000\n",
    "\n",
    "# definindo arquitetura da PMC - com duas camadas intermediárias\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128,128,),  # 2 camadas com 128 neurônios cada\n",
    "                    activation='tanh',              # funcao hiperbolica\n",
    "                    max_iter=epocas,                # numero maximo de iteracoes\n",
    "                    alpha=1e-4,                     # restringir o tamanho dos pesos, fator de penalidade\n",
    "                    solver='lbfgs',                 # algoritmo otimizador (metodo quasi-Newton)\n",
    "                    verbose=False,                  # nao imprimir mensagens de progresso \n",
    "                    tol=1e-5,                       # tolerancia para continuidade da otimizacao\n",
    "                    n_iter_no_change=10,            # a otimizacao vai parar se nos ultimos 'n' resultados não houver melhora\n",
    "                    random_state=1,                 # geracao de numeros aleatorios para inicializacao e pesos de bias \n",
    "                    learning_rate_init=.001)        # taxa de aprendizagem inicial\n",
    "\n",
    "# arrays para armazenar o tempo de duracao, acuracia e precisao de cada treino\n",
    "time_train_mlp =[]\n",
    "acuracia_mlp = []\n",
    "precisao_mlp = []\n",
    "\n",
    "# Validação cruzada com k folds\n",
    "k = 10 \n",
    "skf = StratifiedKFold(n_splits=k, random_state=None)\n",
    "\n",
    "for train_index, test_index in skf.split(x,y): \n",
    "\n",
    "    print(\"Treino: \", train_index)\n",
    "    print(train_index.shape)\n",
    "    print(\"Validacao: \", test_index) \n",
    "    print(test_index.shape)\n",
    "    \n",
    "    sinais_treinamento, sinais_validacao = x[train_index], x[test_index] \n",
    "    labels_treinamento, labels_validacao = y[train_index], y[test_index]\n",
    "    \n",
    "    # para garantir serem randomicos, mas com mesmo padrão/ordem -> 'random_state' igual\n",
    "    x_treinamento, y_treinamento = shuffle(sinais_treinamento, labels_treinamento, random_state = 42)\n",
    "    x_validacao, y_validacao = shuffle(sinais_validacao, labels_validacao, random_state = 42)\n",
    "\n",
    "    # Aqui fazer um treinamento de 100 épocas (parametros da MLP) e uma validacao da MLP\n",
    "    start = process_time()\n",
    "    mlp.fit(x_treinamento, y_treinamento)\n",
    "    end = process_time()\n",
    "    time_mlp = end - start\n",
    "\n",
    "    # Métricas da validacão mlp\n",
    "    preds_val_mlp = mlp.predict(x_validacao)  \n",
    "    cm_val_mlp = confusion_matrix(y_validacao, preds_val_mlp)\n",
    "    print(cm_val_mlp)\n",
    "    \n",
    "    TP = cm_val_mlp[0,0]   # True Positive\n",
    "    FP = cm_val_mlp[0,1]   # False Positive\n",
    "    FN = cm_val_mlp[1,0]   # False Negative \n",
    "    TN = cm_val_mlp[1,1]   # True Negative  \n",
    "\n",
    "    acuracia_mlp_ = (TP+TN)*100/(len(y_validacao))\n",
    "    precisao_mlp_ = TP*100/(TP+FP)\n",
    "    print('Acuracia apurada neste ciclo: ', acuracia_mlp_)\n",
    "    print('Precisao apurada neste ciclo: ', precisao_mlp_)\n",
    "\n",
    "    # Usar no calculo das médias da mlp\n",
    "    time_train_mlp.append(time_mlp)\n",
    "    acuracia_mlp.append(acuracia_mlp_)\n",
    "    precisao_mlp.append(precisao_mlp_)\n",
    "\n",
    "media_time_train_mlp = sum(time_train_mlp) / float(len(time_train_mlp))\n",
    "media_acuracia_mlp = sum(acuracia_mlp) / float(len(acuracia_mlp))\n",
    "media_precisao_mlp = sum(precisao_mlp) / float(len(precisao_mlp))\n",
    "\n",
    "print('--------------------------------------------------------------------------------')\n",
    "print('Tempo médio de treinamento MLP com ' + str(k) + ' kfolds - Tempo: ' + str (media_time_train_mlp) + ' segundos.')\n",
    "print('Médias das Validações com ' + str(k) + ' folds')\n",
    "print('Acurácia_mlp: ' + str(media_acuracia_mlp))\n",
    "print('Precisão_mlp: ' + str(media_precisao_mlp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 06 - Exibir gráfico de perda/loss.\n",
    "# Nao vamos executar pois apenas o otimizador \"sgd\" possui a loss function\n",
    "# (e estamos usando em nossa rede o otimizador \"lbfgs\")\n",
    "\n",
    "# loss_values = mlp.loss_curve_\n",
    "# plt.plot(loss_values)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from time import process_time\n",
    "\n",
    "start_cv = process_time()\n",
    "\n",
    "cv_results = cross_validate(mlp, x_treinamento, y_treinamento, cv=10)\n",
    "\n",
    "end_cv = process_time()\n",
    "time_cv = end_cv - start_cv\n",
    "print('Duracao da CROSS VALIDATION: '  + str (time_cv) + ' segundos.')\n",
    "\n",
    "# Retorna a precisao media a partir dos dados (treino e teste) disponibilizados\n",
    "cv_results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'activation':('identity', 'logistic', 'tanh', 'relu'), 'tol':[1e-5, 1e-1]}\n",
    "\n",
    "clf = GridSearchCV(mlp, parameters, cv=10)\n",
    "clf.fit(x_treinamento, y_treinamento,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_['params'][clf.best_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
